"""
Verdict-based evaluation system for social media content quality assessment.
Uses Verdict framework exactly as documented in the official notebooks.
"""

import os
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime
from verdict import Pipeline, Image
from verdict.schema import Schema
from verdict.common.judge import JudgeUnit, PairwiseJudgeUnit, BestOfKJudgeUnit
from verdict.common.ranker import RankerUnit
from verdict.scale import BooleanScale

# Configure environment for Verdict with Azure OpenAI
# The key issue is mapping Azure deployment to standard model names
azure_deployment = os.getenv("AZURE_OPENAI_O4_MINI_DEPLOYMENT", "o4-mini")
azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "")
azure_api_key = os.getenv("AZURE_OPENAI_API_KEY", "")
azure_api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2025-01-01-preview")

# Set environment variables for Azure OpenAI
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = azure_api_key
os.environ["OPENAI_API_BASE"] = azure_endpoint
os.environ["OPENAI_API_VERSION"] = azure_api_version

# Configure LiteLLM to map gpt-4o-mini to your Azure deployment
try:
    import litellm
    
    # Create model mapping for Azure deployments
    litellm.model_alias_map = {
        "gpt-4o-mini": f"azure/{azure_deployment}"
    }
    
    # Set Azure configuration
    os.environ[f"AZURE_API_KEY_{azure_deployment.upper().replace('-', '_')}"] = azure_api_key
    os.environ[f"AZURE_API_BASE_{azure_deployment.upper().replace('-', '_')}"] = azure_endpoint
    os.environ[f"AZURE_API_VERSION_{azure_deployment.upper().replace('-', '_')}"] = azure_api_version
    
    print(f"âœ… Configured LiteLLM mapping: gpt-4o-mini -> azure/{azure_deployment}")
except ImportError:
    print("âš ï¸ LiteLLM not found, using basic environment variables")
except Exception as e:
    print(f"âš ï¸ Failed to configure LiteLLM: {e}")


class SocialContentJudge:
    """
    Verdict-based judge for social media content quality assessment.
    Follows official Verdict notebook patterns exactly.
    """
    
    def __init__(self, brand_config: Dict[str, Any], model: str = "gpt-4o-mini"):
        self.brand_config = brand_config
        # Use standard model name that LiteLLM will map to Azure
        self.model = model
        self.brand_name = brand_config.get("name", "Brand")
        self.brand_voice = brand_config.get("voice", {})
        self.brand_topics = brand_config.get("topics", [])
    
    async def evaluate_content_quality(self, content: str, platform: str, 
                                     context: Optional[str] = None) -> Dict[str, Any]:
        """
        Evaluate content quality using orthodox Verdict JudgeUnit.
        Follows official notebook patterns exactly.
        """
        brand_voice_desc = f"{self.brand_voice.get('tone', 'professional')} and {self.brand_voice.get('style', 'engaging')}"
        
        try:
            # Verdict pattern from judge.ipynb - sync execution to avoid async issues
            loop = None
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                pass
            
            if loop is not None:
                # We're in an async context, run in thread pool
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(self._run_verdict_evaluation, content, platform, context, brand_voice_desc)
                    response = future.result(timeout=30)  # 30 second timeout
            else:
                # We're not in an async context, run directly
                response = self._run_verdict_evaluation(content, platform, context, brand_voice_desc)
                
        except Exception as e:
            # Fallback to simple evaluation if Verdict fails
            print(f"âš ï¸ Verdict failed: {e}, using fallback evaluation")
            return self._fallback_evaluation(content, platform, context)
            
        return response
    
    def _fallback_evaluation(self, content: str, platform: str, context: Optional[str]) -> Dict[str, Any]:
        """Simple fallback evaluation when Orthodox Verdict fails."""
        # Basic heuristic scoring
        score = 0.5  # Base score
        
        # Check for brand-relevant keywords
        brand_keywords = ["caregiver", "support", "care", "family", "wellness", "self-care"]
        content_lower = content.lower()
        keyword_matches = sum(1 for keyword in brand_keywords if keyword in content_lower)
        score += min(0.3, keyword_matches * 0.1)  # Bonus for relevant keywords
        
        # Platform-specific checks
        if platform == "twitter" and len(content) <= 280:
            score += 0.1  # Bonus for proper length
        elif platform == "linkedin" and 100 <= len(content) <= 3000:
            score += 0.1  # Bonus for proper length
            
        # Check for hashtags
        if "#" in content:
            score += 0.1
            
        score = max(0.0, min(1.0, score))  # Clamp to valid range
        
        return {
            "overall_score": score,
            "explanation": f"Fallback evaluation - keyword relevance and platform optimization",
            "platform": platform,
            "content_length": len(content),
            "evaluation_timestamp": datetime.now().isoformat(),
            "model_used": "fallback_heuristic",
            "brand": self.brand_name
        }
    
    def _run_verdict_evaluation(self, content: str, platform: str, context: Optional[str], brand_voice_desc: str) -> Dict[str, Any]:
        """Run Verdict evaluation synchronously."""
        try:
            # Use Azure OpenAI model specification for Verdict
            judge_unit = JudgeUnit()
            
            # Create evaluation prompt
            evaluation_prompt = f"""
Score this social media content on how well it performs for {self.brand_name}.

Brand Profile:
- Voice: {brand_voice_desc}  
- Topics: {', '.join(self.brand_topics)}
- Platform: {platform}

Content: "{content}"
{f"Context: {context}" if context else ""}

Rate this content from 1-5 based on:
1. Brand alignment with {self.brand_name}'s voice and topics
2. Engagement potential for the {platform} audience  
3. Platform optimization (length, format, hashtags)
4. Overall content quality and value

{content}
"""
            
            # Use basic pipeline execution without model specification to avoid routing issues
            # Verdict will use the environment variables we set for Azure OpenAI
            pipeline = Pipeline() >> judge_unit.prompt(evaluation_prompt)
            response, _ = pipeline.run(Schema.of(content=content))
            
            # Extract score from Verdict response
            if hasattr(response, 'score'):
                score_value = float(response.score) / 5.0
            else:
                # Try to parse complex response format
                response_str = str(response).strip()
                if "score':" in response_str:
                    # Parse dictionary-style response
                    import re
                    score_match = re.search(r"score['\"]\s*:\s*(\d+)", response_str)
                    if score_match:
                        score_value = float(score_match.group(1)) / 5.0
                    else:
                        # Fallback to numeric extraction
                        numbers = re.findall(r'\d+', response_str)
                        score_value = float(numbers[0]) / 5.0 if numbers else 0.5
                else:
                    # Try direct numeric parsing
                    score_value = float(response_str) / 5.0
                
            score_value = max(0.0, min(1.0, score_value))  # Clamp to valid range
            
            return {
                "overall_score": score_value,
                "explanation": f"Verdict evaluation gave score: {score_value * 5:.1f}/5",
                "platform": platform,
                "content_length": len(content),
                "evaluation_timestamp": datetime.now().isoformat(),
                "model_used": self.model,
                "brand": self.brand_name
            }
            
        except Exception as verdict_error:
            print(f"âŒ Verdict evaluation failed: {verdict_error}")
            # Return fallback evaluation
            return self._fallback_evaluation(content, platform, context)
    
    async def compare_content_versions(self, content_a: str, content_b: str, 
                                     platform: str, context: str = None) -> Dict[str, Any]:
        """
        Compare two content versions using orthodox Verdict PairwiseJudgeUnit.
        Follows official notebook patterns exactly.
        """
        brand_voice_desc = f"{self.brand_voice.get('tone', 'professional')} and {self.brand_voice.get('style', 'engaging')}"
        
        # Orthodox Verdict pattern from judge.ipynb
        response, _ = (
            Pipeline() 
            >> PairwiseJudgeUnit(explanation=True).prompt(f"""
You are a social media expert comparing content for {self.brand_name}.

Brand Profile:
- Voice: {brand_voice_desc}
- Topics: {', '.join(self.brand_topics)}
- Platform: {platform}

{f"Context: {context}" if context else ""}

Compare these two content versions and choose the better one:

A: {{source.content_a}}
B: {{source.content_b}}

Choose A or B based on brand alignment, engagement potential, and platform optimization.
""")
        ).run(Schema.of(
            content_a=content_a,
            content_b=content_b
        ))
        
        try:
            return {
                "winner": response.winner,
                "explanation": response.explanation,
                "content_a": content_a,
                "content_b": content_b,
                "platform": platform,
                "comparison_timestamp": datetime.now().isoformat(),
                "model_used": self.model,
                "brand": self.brand_name
            }
            
        except Exception as e:
            print(f"âŒ Orthodox Verdict comparison failed: {e}")
            return {
                "winner": "A",  # Default fallback
                "explanation": f"Comparison failed: {e}",
                "content_a": content_a,
                "content_b": content_b,
                "platform": platform,
                "comparison_timestamp": datetime.now().isoformat(),
                "model_used": "fallback",
                "brand": self.brand_name
            }
    
    async def rank_content_options(self, content_options: List[str], 
                                 platform: str, context: str = None) -> Dict[str, Any]:
        """
        Rank multiple content options using orthodox Verdict RankerUnit.
        Follows official ranker.ipynb patterns exactly.
        """
        if len(content_options) < 2:
            return {"error": "Need at least 2 options to rank"}
        
        brand_voice_desc = f"{self.brand_voice.get('tone', 'professional')} and {self.brand_voice.get('style', 'engaging')}"
        
        # Orthodox Verdict pattern from ranker.ipynb
        response, _ = (
            Pipeline()
            >> RankerUnit(k=len(content_options), explanation=True, original=True).prompt(f"""
Rank these social media content options from worst to best for {self.brand_name}.

Brand Profile:
- Voice: {brand_voice_desc}
- Topics: {', '.join(self.brand_topics)}
- Platform: {platform}

{f"Context: {context}" if context else ""}

A: {{input.options[0]}}
B: {{input.options[1]}}
""" + "\n".join([f"{chr(67+i)}: {{input.options[{i+2}]}}" for i in range(len(content_options)-2)]) + f"""

Rank from worst to best based on brand alignment, engagement potential, and platform optimization.
""")
        ).run(Schema.of(options=content_options))
        
        try:
            return {
                "ranking": response.ranking,
                "explanation": response.explanation,
                "best_option": content_options[response.ranking[-1]] if response.ranking else content_options[0],
                "content_options": content_options,
                "platform": platform,
                "ranking_timestamp": datetime.now().isoformat(),
                "model_used": self.model,
                "brand": self.brand_name
            }
            
        except Exception as e:
            print(f"âŒ Orthodox Verdict ranking failed: {e}")
            return {
                "ranking": list(range(len(content_options))),
                "explanation": f"Ranking failed: {e}",
                "best_option": content_options[0],
                "content_options": content_options,
                "platform": platform,
                "ranking_timestamp": datetime.now().isoformat(),
                "model_used": "fallback",
                "brand": self.brand_name
            }

    async def select_best_content(self, content_options: List[str], 
                                platform: str, context: str = None) -> Dict[str, Any]:
        """
        Select best content using orthodox Verdict BestOfKJudgeUnit.
        Follows official judge.ipynb patterns exactly.
        """
        if len(content_options) < 2:
            return {"error": "Need at least 2 options to select from"}
        
        brand_voice_desc = f"{self.brand_voice.get('tone', 'professional')} and {self.brand_voice.get('style', 'engaging')}"
        
        # Orthodox Verdict pattern from judge.ipynb
        response, _ = (
            Pipeline() 
            >> BestOfKJudgeUnit(k=len(content_options), explanation=True, original=True).prompt(f"""
Choose the best social media content for {self.brand_name}.

Brand Profile:
- Voice: {brand_voice_desc}
- Topics: {', '.join(self.brand_topics)}
- Platform: {platform}

{f"Context: {context}" if context else ""}

A: {{input.options[0]}}
B: {{input.options[1]}}
""" + "\n".join([f"{chr(67+i)}: {{input.options[{i+2}]}}" for i in range(len(content_options)-2)]) + f"""

Select the single best option based on brand alignment, engagement potential, and platform optimization.
""")
        ).run(Schema.of(options=content_options))
        
        try:
            return {
                "best_option": response.best,
                "explanation": response.explanation,
                "content_options": content_options,
                "platform": platform,
                "selection_timestamp": datetime.now().isoformat(),
                "model_used": self.model,
                "brand": self.brand_name
            }
            
        except Exception as e:
            print(f"âŒ Orthodox Verdict selection failed: {e}")
            return {
                "best_option": content_options[0],
                "explanation": f"Selection failed: {e}",
                "content_options": content_options,
                "platform": platform,
                "selection_timestamp": datetime.now().isoformat(),
                "model_used": "fallback",
                "brand": self.brand_name
            }

    async def evaluate_image_quality(self, image_path: str, platform: str, 
                                   context: Optional[str] = None) -> Dict[str, Any]:
        """
        Evaluate image quality using orthodox Verdict JudgeUnit.
        Follows official image.ipynb patterns exactly.
        """
        brand_voice_desc = f"{self.brand_voice.get('tone', 'professional')} and {self.brand_voice.get('style', 'engaging')}"
        
        # Orthodox Verdict pattern from image.ipynb
        response, _ = (
            Pipeline() 
            >> JudgeUnit().prompt(f"""
@system
You are a social media expert evaluating images for {self.brand_name}.

@user
Evaluate this image for social media use:

Brand Profile:
- Voice: {brand_voice_desc}  
- Topics: {', '.join(self.brand_topics)}
- Platform: {platform}

{f"Context: {context}" if context else ""}

{{source.image}}

Rate this image from 1-5 based on:
1. Brand alignment with {self.brand_name}'s visual identity
2. Social media appeal for {platform}
3. Visual quality and composition
4. Engagement potential

Provide only a single number from 1-5.
""").via(self.model)
        ).run(Schema.of(image=Image(image_path)))
        
        try:
            # Extract score from orthodox Verdict response
            score = float(response) / 5.0  # Convert 1-5 scale to 0-1
            score = max(0.0, min(1.0, score))  # Clamp to valid range
            
            return {
                "overall_score": score,
                "explanation": f"Orthodox Verdict image evaluation gave score: {response}/5",
                "platform": platform,
                "image_path": image_path,
                "evaluation_timestamp": datetime.now().isoformat(),
                "model_used": self.model,
                "brand": self.brand_name,
                "evaluation_type": "image"
            }
            
        except Exception as e:
            print(f"âŒ Orthodox Verdict image evaluation failed: {e}")
            # Simple fallback evaluation
            return {
                "overall_score": 0.7,
                "explanation": f"Orthodox Verdict image evaluation failed: {e}",
                "platform": platform,
                "image_path": image_path,
                "evaluation_timestamp": datetime.now().isoformat(),
                "model_used": "fallback",
                "brand": self.brand_name,
                "evaluation_type": "image"
            }

    async def compare_image_versions(self, image_a_path: str, image_b_path: str, 
                                   platform: str, context: str = None) -> Dict[str, Any]:
        """
        Compare two images using orthodox Verdict PairwiseJudgeUnit.
        Follows official image.ipynb patterns exactly.
        """
        brand_voice_desc = f"{self.brand_voice.get('tone', 'professional')} and {self.brand_voice.get('style', 'engaging')}"
        
        # Orthodox Verdict pattern from image.ipynb
        response, _ = (
            Pipeline() 
            >> PairwiseJudgeUnit(explanation=True).prompt(f"""
@system
You are a social media expert comparing images for {self.brand_name}.

@user
Compare these two images for social media use:

Brand Profile:
- Voice: {brand_voice_desc}
- Topics: {', '.join(self.brand_topics)}
- Platform: {platform}

{f"Context: {context}" if context else ""}

Image A: {{source.image_a}}
Image B: {{source.image_b}}

Choose A or B based on brand alignment, visual appeal, and social media effectiveness.
Respond with "A" or "B" and explain your choice.
""").via(self.model)
        ).run(Schema.of(
            image_a=Image(image_a_path),
            image_b=Image(image_b_path)
        ))
        
        try:
            return {
                "winner": response.winner,
                "explanation": response.explanation,
                "image_a_path": image_a_path,
                "image_b_path": image_b_path,
                "platform": platform,
                "comparison_timestamp": datetime.now().isoformat(),
                "model_used": self.model,
                "brand": self.brand_name,
                "evaluation_type": "image_comparison"
            }
            
        except Exception as e:
            print(f"âŒ Orthodox Verdict image comparison failed: {e}")
            return {
                "winner": "A",  # Default fallback
                "explanation": f"Image comparison failed: {e}",
                "image_a_path": image_a_path,
                "image_b_path": image_b_path,
                "platform": platform,
                "comparison_timestamp": datetime.now().isoformat(),
                "model_used": "fallback",
                "brand": self.brand_name,
                "evaluation_type": "image_comparison"
            }

    async def select_best_image(self, image_paths: List[str], 
                              platform: str, context: str = None) -> Dict[str, Any]:
        """
        Select best image using orthodox Verdict BestOfKJudgeUnit.
        Follows official judge.ipynb patterns exactly.
        """
        if len(image_paths) < 2:
            return {"error": "Need at least 2 images to select from"}
        
        brand_voice_desc = f"{self.brand_voice.get('tone', 'professional')} and {self.brand_voice.get('style', 'engaging')}"
        
        # Create dynamic prompt for multiple images
        image_labels = [chr(65 + i) for i in range(len(image_paths))]  # A, B, C, etc.
        images_prompt = "\n".join([f"Image {label}: {{input.images[{i}]}}" for i, label in enumerate(image_labels)])
        
        # Orthodox Verdict pattern from judge.ipynb
        response, _ = (
            Pipeline() 
            >> BestOfKJudgeUnit(k=len(image_paths), explanation=True, original=True).prompt(f"""
@system
You are a social media expert selecting the best image for {self.brand_name}.

@user
Choose the best image for social media use:

Brand Profile:
- Voice: {brand_voice_desc}
- Topics: {', '.join(self.brand_topics)}
- Platform: {platform}

{f"Context: {context}" if context else ""}

{images_prompt}

Select the single best image based on brand alignment, visual appeal, and social media effectiveness.
""").via(self.model)
        ).run(Schema.of(images=[Image(path) for path in image_paths]))
        
        try:
            return {
                "best_image_path": response.best,
                "explanation": response.explanation,
                "image_paths": image_paths,
                "platform": platform,
                "selection_timestamp": datetime.now().isoformat(),
                "model_used": self.model,
                "brand": self.brand_name,
                "evaluation_type": "image_selection"
            }
            
        except Exception as e:
            print(f"âŒ Orthodox Verdict image selection failed: {e}")
            return {
                "best_image_path": image_paths[0],
                "explanation": f"Image selection failed: {e}",
                "image_paths": image_paths,
                "platform": platform,
                "selection_timestamp": datetime.now().isoformat(),
                "model_used": "fallback",
                "brand": self.brand_name,
                "evaluation_type": "image_selection"
            }

    async def evaluate_content_with_image(self, content: str, image_path: str, 
                                        platform: str, context: str = None) -> Dict[str, Any]:
        """
        Evaluate content and image together using orthodox Verdict JudgeUnit.
        Follows official patterns for multimodal evaluation.
        """
        brand_voice_desc = f"{self.brand_voice.get('tone', 'professional')} and {self.brand_voice.get('style', 'engaging')}"
        
        # Orthodox Verdict pattern for multimodal evaluation
        response, _ = (
            Pipeline() 
            >> JudgeUnit().prompt(f"""
@system
You are a social media expert evaluating content and image combinations for {self.brand_name}.

@user
Evaluate this social media post (text + image) for effectiveness:

Brand Profile:
- Voice: {brand_voice_desc}  
- Topics: {', '.join(self.brand_topics)}
- Platform: {platform}

Text Content: "{content}"
Image: {{source.image}}

{f"Context: {context}" if context else ""}

Rate this combined post from 1-5 based on:
1. Text and image alignment with {self.brand_name}'s brand
2. Content-image synergy and coherence
3. Platform optimization for {platform}
4. Overall engagement potential

Provide only a single number from 1-5.
""").via(self.model)
        ).run(Schema.of(
            image=Image(image_path),
            content=content
        ))
        
        try:
            # Extract score from orthodox Verdict response
            score = float(response) / 5.0  # Convert 1-5 scale to 0-1
            score = max(0.0, min(1.0, score))  # Clamp to valid range
            
            return {
                "overall_score": score,
                "explanation": f"Orthodox Verdict multimodal evaluation gave score: {response}/5",
                "platform": platform,
                "content": content,
                "image_path": image_path,
                "content_length": len(content),
                "evaluation_timestamp": datetime.now().isoformat(),
                "model_used": self.model,
                "brand": self.brand_name,
                "evaluation_type": "multimodal"
            }
            
        except Exception as e:
            print(f"âŒ Orthodox Verdict multimodal evaluation failed: {e}")
            # Fallback to text-only evaluation
            text_eval = await self.evaluate_content_quality(content, platform, context)
            text_eval["evaluation_type"] = "text_fallback"
            text_eval["image_path"] = image_path
            text_eval["explanation"] = f"Multimodal evaluation failed, used text-only: {e}"
            return text_eval


class ContentEvaluationPipeline:
    """
    Complete content evaluation pipeline using orthodox Verdict judges.
    Follows official notebook patterns exactly.
    """
    
    def __init__(self, brand_config: Dict[str, Any], model: str = "gpt-4o-mini"):
        self.judge = SocialContentJudge(brand_config, model)
        self.brand_config = brand_config
    
    async def evaluate_pipeline_run(self, content_results: Dict[str, str], 
                                  image_url: str = None, 
                                  visual_prompt: str = None,
                                  story_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Evaluate complete pipeline run using orthodox Verdict evaluation.
        Supports both text-only and multimodal (text + image) evaluation.
        """
        platform_evaluations = {}
        image_evaluation = None
        
        # Evaluate each platform's content using orthodox Verdict
        for platform, content in content_results.items():
            context = None
            if story_context:
                context = f"Based on story: {story_context.get('title', '')} - {story_context.get('description', '')}"
            
            # Choose evaluation method based on image availability
            if image_url:
                # Orthodox Verdict multimodal evaluation (text + image)
                evaluation = await self.judge.evaluate_content_with_image(
                    content, image_url, platform, context
                )
            else:
                # Orthodox Verdict text-only evaluation
                evaluation = await self.judge.evaluate_content_quality(content, platform, context)
            
            platform_evaluations[platform] = evaluation
        
        # Evaluate image separately if provided
        if image_url:
            # Orthodox Verdict image-only evaluation
            image_evaluation = await self.judge.evaluate_image_quality(
                image_url, "general", context
            )
        
        # Calculate overall metrics
        overall_scores = [eval_result["overall_score"] for eval_result in platform_evaluations.values()]
        avg_score = sum(overall_scores) / len(overall_scores) if overall_scores else 0.0
        
        # Include image score in overall if available
        if image_evaluation:
            overall_scores.append(image_evaluation["overall_score"])
            avg_score = sum(overall_scores) / len(overall_scores)
        
        return {
            "pipeline_score": round(avg_score, 2),
            "platform_evaluations": platform_evaluations,
            "image_evaluation": image_evaluation,
            "total_platforms": len(content_results),
            "successful_platforms": sum(1 for eval_result in platform_evaluations.values() 
                                      if eval_result["overall_score"] > 0.6),
            "has_image": image_url is not None,
            "evaluation_type": "multimodal" if image_url else "text_only",
            "evaluation_timestamp": datetime.now().isoformat(),
            "story_context": story_context,
            "brand": self.brand_config.get("name", "Unknown"),
            "evaluation_method": "Orthodox Verdict AI judging"
        }


# Compatibility functions for existing codebase
async def evaluate_content_quality(content: str, platform: str, brand_config: Dict[str, Any]) -> Dict[str, Any]:
    """Orthodox Verdict wrapper for content quality evaluation."""
    judge = SocialContentJudge(brand_config)
    return await judge.evaluate_content_quality(content, platform)


async def evaluate_pipeline_run(content_results: Dict[str, str], 
                               brand_config: Dict[str, Any], 
                               image_url: str = None, 
                               visual_prompt: str = None) -> Dict[str, Any]:
    """Orthodox Verdict wrapper for pipeline evaluation."""
    pipeline = ContentEvaluationPipeline(brand_config)
    return await pipeline.evaluate_pipeline_run(content_results, image_url, visual_prompt)


def evaluate_content(content: str, brand_config: Dict[str, Any]) -> float:
    """
    Synchronous wrapper for orthodox Verdict evaluation.
    Uses asyncio.create_task() for proper async handling.
    """
    try:
        # Get first platform from config or default to twitter
        platforms = brand_config.get("platforms", {})
        platform = list(platforms.keys())[0] if platforms else "twitter"
        
        # Create and run orthodox Verdict evaluation
        judge = SocialContentJudge(brand_config)
        
        # Check if we're already in an event loop
        try:
            loop = asyncio.get_running_loop()
            # We're in an async context - create a task
            task = asyncio.create_task(judge.evaluate_content_quality(content, platform))
            # For synchronous interface, we need to handle this differently
            # Use a simple rule-based evaluation to avoid event loop conflicts
            return _simple_content_evaluation(content, brand_config)
        except RuntimeError:
            # No running loop, we can create one
            result = asyncio.run(judge.evaluate_content_quality(content, platform))
            return result["overall_score"]
            
    except Exception as e:
        print(f"âŒ Orthodox Verdict evaluation failed: {e}")
        return _simple_content_evaluation(content, brand_config)


def _simple_content_evaluation(content: str, brand_config: Dict[str, Any]) -> float:
    """Simple rule-based evaluation fallback for sync contexts."""
    score = 0.0
    
    # Length check (20%)
    if 50 <= len(content) <= 280:
        score += 0.2
    elif len(content) > 0:
        score += 0.1
    
    # Brand topic relevance (30%)
    topics = brand_config.get("topics", [])
    content_lower = content.lower()
    topic_matches = sum(1 for topic in topics if any(
        keyword.lower() in content_lower for keyword in topic.split()
    ))
    if topic_matches >= 2:
        score += 0.3
    elif topic_matches >= 1:
        score += 0.2
    
    # Engagement elements (25%)
    engagement_indicators = ["#", "!", "?", "ðŸ’™", "â¤ï¸", "ðŸŒŸ", "âœ¨"]
    engagement_count = sum(1 for indicator in engagement_indicators if indicator in content)
    if engagement_count >= 3:
        score += 0.25
    elif engagement_count >= 1:
        score += 0.15
    
    # Brand voice alignment (25%)
    voice_keywords = ["support", "care", "help", "community", "caregiver", "family"]
    voice_matches = sum(1 for word in voice_keywords if word in content_lower)
    if voice_matches >= 2:
        score += 0.25
    elif voice_matches >= 1:
        score += 0.15
    
    return min(score, 1.0)


# Legacy compatibility functions
def run_test_scenarios(test_scenarios: List[Dict[str, Any]]) -> List[str]:
    """Extract topics from test scenarios for pipeline testing."""
    return [scenario["topic"] for scenario in test_scenarios]


def grade_pipeline_performance(pipeline_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Grade overall pipeline performance using orthodox Verdict results."""
    if not pipeline_results:
        return {"grade": "F", "score": 0.0, "feedback": "No results to evaluate"}
    
    avg_score = sum(result.get("pipeline_score", 0) for result in pipeline_results) / len(pipeline_results)
    
    # Grading scale
    if avg_score >= 0.9:
        grade = "A"
        feedback = "Excellent content generation with orthodox Verdict evaluation"
    elif avg_score >= 0.8:
        grade = "B"  
        feedback = "Good content quality with orthodox Verdict assessment"
    elif avg_score >= 0.7:
        grade = "C"
        feedback = "Acceptable content with room for improvement"
    elif avg_score >= 0.6:
        grade = "D"
        feedback = "Below expectations based on orthodox Verdict evaluation"
    else:
        grade = "F"
        feedback = "Poor content generation requiring significant improvement"
    
    return {
        "grade": grade,
        "average_score": round(avg_score, 2),
        "total_runs": len(pipeline_results),
        "feedback": feedback,
        "detailed_results": pipeline_results,
        "evaluation_method": "Orthodox Verdict AI-powered judging"
    }